# Polars Eval Metrics

[![Tests](https://github.com/elong0527/demo_eval_metric/actions/workflows/test.yml/badge.svg)](https://github.com/elong0527/demo_eval_metric/actions/workflows/test.yml)
[![Documentation](https://github.com/elong0527/demo_eval_metric/actions/workflows/docs.yml/badge.svg)](https://github.com/elong0527/demo_eval_metric/actions/workflows/docs.yml)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)

A high-performance model evaluation framework built on Polars lazy evaluation.

## Documentation

Visit our [documentation website](https://elong0527.github.io/demo_eval_metric/) for detailed guides and examples.

## Features

- **Fast**: Leverages Polars lazy evaluation for optimal performance
- **Flexible**: Support for custom metrics and expressions
- **Type-safe**: Pydantic models with validation
- **Simple**: Clean API with sensible defaults
- **Extensible**: Easy to add new metrics and aggregation types

## Installation

```bash
# Install from source
pip install -e .

# Install with development dependencies
pip install -e ".[dev]"

# Install with all dependencies (dev, test, docs)
pip install -e ".[dev,test,docs]"
```

## Quick Start

```python
from polars_eval_metrics import MetricEvaluator, create_metrics
import polars as pl

# Create metrics from simple names or configurations
metrics = create_metrics(['mae', 'rmse'])

# Or from detailed configurations
# metrics = create_metrics([
#     {'name': 'mae', 'label': 'Mean Absolute Error'},
#     {'name': 'rmse', 'label': 'Root Mean Squared Error'}
# ])

# Create sample data
df = pl.DataFrame({
    "actual": [1.0, 2.0, 3.0, 4.0],
    "model1": [1.1, 2.2, 2.9, 4.1],
    "model2": [0.9, 2.1, 3.2, 3.8],
    "treatment": ["A", "A", "B", "B"]
})

# Evaluate
evaluator = MetricEvaluator(
    df=df,
    metrics=metrics,
    ground_truth="actual",
    estimates=["model1", "model2"],
    group_by=["treatment"]
)

results = evaluator.evaluate()
print(results)
```

## Code Coverage

We maintain comprehensive test coverage to ensure code quality. Coverage reports are generated by GitHub Actions and can be downloaded from the [test workflow artifacts](https://github.com/elong0527/demo_eval_metric/actions/workflows/test.yml).

## Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src/polars_eval_metrics --cov-report=html

# View coverage report
open htmlcov/index.html
```

## Documentation Development

```bash
# Install documentation dependencies
pip install -e ".[docs]"

# Build documentation locally
cd docs
quarto render

# Preview documentation
quarto preview
```

## > Contributing

Contributions are welcome! Please:

1. Write tests for new features
2. Ensure all tests pass
3. Maintain or improve code coverage
4. Follow the existing code style

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## = Links

- [Documentation](https://elong0527.github.io/demo_eval_metric/)
- [GitHub Repository](https://github.com/elong0527/demo_eval_metric)
- [Issue Tracker](https://github.com/elong0527/demo_eval_metric/issues)
