---
title: "MetricEvaluator: Evaluation Pipeline"
---

# MetricEvaluator

The `MetricEvaluator` class is the main evaluation engine that executes metric computations on your data. 
It handles hierarchical aggregations, grouping, filtering, and manages the entire evaluation pipeline using Polars' lazy evaluation for optimal performance.

`MetricEvaluator.evaluate()` returns an `EvaluationResult`, which behaves like a
Polars `DataFrame` while exposing helpers (`collect()`, `to_ard()`) for
retrieving the canonical ARD representation when needed.

# Example Data

```{python}
import polars as pl
from polars_eval_metrics import MetricDefine, MetricEvaluator
from data_generator import generate_sample_data

# Create sample data using shared generator
data = generate_sample_data(n_subjects=6, n_visits=3, n_groups=2)

data
```

# Examples 

## Simple Example 

```{python}
# Define metrics
metrics = [
    MetricDefine(name="mae", label="Mean Absolute Error"),
    MetricDefine(name="rmse", label="Root Mean Squared Error"),
]

# Create evaluator and run evaluation
evaluator = MetricEvaluator(
    df=data,
    metrics=metrics,
    ground_truth="actual",
    estimates=["model1", "model2"],
)

evaluator.evaluate()
```

## Real Example 

```{python}
# Define metrics
metrics = [
    MetricDefine(name="n_subject", label = "Number of Subjects", scope = "global"), 
    MetricDefine(name="n_sample", label = "Number of Samples", scope = "group"), 
    MetricDefine(name="pct_sample_with_data", label = "Percent of Samples with Data", scope = "group"),
    MetricDefine(name="mae", label="MAE"),
    MetricDefine(name="rmse", label="RMSE"),
]

# Create evaluator and run evaluation
evaluator = MetricEvaluator(
    df=data,
    metrics=metrics,
    ground_truth="actual",
    estimates=["model1", "model2"],
    group_by=["treatment"]
)

res = evaluator.evaluate()
res
```

The resulting `DataFrame` keeps the formatted `value` column alongside the underlying `stat` struct, so you can render metrics directly while still accessing typed values through `res["stat"]` when needed.

```{python}
# Grab integer counts for the subject metrics
res.filter(pl.col("metric") == "n_subject").with_columns(
    pl.col("stat").struct.field("value_int").alias("subject_count")
)
```

```{python}
res.filter(pl.col("scope") == "global").pivot( 
    on = ["label"], 
    index = "treatment",
    values = "value")
```

```{python}
res.filter(pl.col("scope") == "group").pivot( 
    on = ["label"], 
    index = "treatment",
    values = "value")
```

```{python}
res.filter(pl.col("scope").is_null()).pivot( 
    on = ["estimate", "label"], 
    index = "treatment",
    values = "value")
```

# Subject-level Metrics

## Within Subject Metrics

```{python}
within_subject_metrics = MetricDefine(
        name="mae",
        type="within_subject",
        label="MAE per Subject"
    )


evaluator = MetricEvaluator(
    df=data,
    metrics=within_subject_metrics,
    ground_truth="actual",
    estimates=["model1", "model2"],
)

evaluator.evaluate()
```


## Across subject metrics

```{python}
across_subject_metrics = MetricDefine(
        name="mae:mean", 
        type="across_subject",
        label="Mean of Subject MAEs"
    )

evaluator = MetricEvaluator(
    df=data,
    metrics=across_subject_metrics,
    ground_truth="actual",
    estimates=["model1", "model2"],
)

evaluator.evaluate()
```

## Combining within and between subject metrics 

```{python}
# Combine the metric lists using list concatenation or unpacking
evaluator = MetricEvaluator(
    df=data,
    metrics=[
        within_subject_metrics,
        across_subject_metrics
    ],
    ground_truth="actual",
    estimates=["model1", "model2"],
)

evaluator.evaluate()
```

# Visit-level Metrics

## Within Visit Metrics

```{python}
within_visit_metrics = MetricDefine(
        name="mae",
        type="within_visit",
        label="MAE per Visit"
    )

evaluator = MetricEvaluator(
    df=data,
    metrics=within_visit_metrics,
    ground_truth="actual",
    estimates=["model1", "model2"],
)

evaluator.evaluate()
```

## Across Visit Metrics

```{python}
across_visit_metrics = MetricDefine(
        name="mae:mean", 
        type="across_visit",
        label="Mean of Visit MAEs"
    )

evaluator = MetricEvaluator(
    df=data,
    metrics=across_visit_metrics,
    ground_truth="actual",
    estimates=["model1", "model2"],
)

evaluator.evaluate()
```

## Combining within and between visit metrics 

```{python}
evaluator = MetricEvaluator(
    df=data,
    metrics=[
        within_visit_metrics,
        across_visit_metrics,
    ],
    ground_truth="actual",
    estimates=["model1", "model2"],
)

evaluator.evaluate()
```

# Scope 

## Global Scope 

Global scope metrics compute a single value across the entire dataset, ignoring model and group distinctions.

```{python}
global_scope_metrics = MetricDefine(
    name = "n_subject", 
    scope = "global"
)

evaluator = MetricEvaluator(
    df=data,
    metrics=[
        global_scope_metrics,
    ],
    ground_truth="actual",
    estimates=["model1", "model2"],
)

evaluator.evaluate()
```

## Group Scope

Group scope metrics compute one value per group, aggregating across all models.

```{python}
group_scope_metrics = MetricDefine(
    name = "n_subject", 
    scope = "group"
)

evaluator = MetricEvaluator(
    df=data,
    metrics=[group_scope_metrics],
    ground_truth="actual",
    estimates=["model1", "model2"],
    group_by=["treatment"],
)

evaluator.evaluate()
```

## Model Scope

Model scope metrics compute one value per model, ignoring group distinctions.

```{python}
model_scope_metrics = MetricDefine(
    name = "n_sample_with_data", 
    scope = "model"
)

evaluator = MetricEvaluator(
    df=data,
    metrics=[model_scope_metrics],
    ground_truth="actual",
    estimates=["model1", "model2"],
    group_by=["treatment"],
)

evaluator.evaluate()
```

# Grouping and Stratification

## Group-By Analysis

```{python}
# Evaluate metrics by treatment group
evaluator = MetricEvaluator(
    df=data,
    metrics=[MetricDefine(name="mae"), MetricDefine(name="rmse")],
    ground_truth="actual",
    estimates=["model1", "model2"],
    group_by=["treatment"],
)

evaluator.evaluate()
```

## Subgroup Analysis

```{python}
# Evaluate with subgroup stratification (gender and race already in data)
evaluator = MetricEvaluator(
    df=data,
    metrics=[MetricDefine(name="mae")],
    ground_truth="actual",
    estimates=["model1"],
    group_by=["treatment"],
    subgroup_by=["gender", "race"],
)

evaluator.evaluate()
```
