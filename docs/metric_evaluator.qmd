---
title: "MetricEvaluator: Evaluation Pipeline"
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: false
---

# MetricEvaluator

The `MetricEvaluator` class is the main evaluation engine that executes metric computations on your data. 
It handles hierarchical aggregations, grouping, filtering, and manages the entire evaluation pipeline using Polars' lazy evaluation for optimal performance.

# Example Data

```{python}
import polars as pl
from polars_eval_metrics import MetricDefine, MetricEvaluator, MetricType

# Create sample data
data = pl.DataFrame({
    "subject_id": [1, 1, 1, 2, 2, 2, 3, 3, 3],
    "visit_id": [1, 2, 3, 1, 2, 3, 1, 2, 3],
    "treatment": ["A", "A", "A", "A", "A", "A", "B", "B", "B"],
    "age_group": ["young", "young", "young", "middle", "middle", "middle", "senior", "senior", "senior"],
    "sex": ["M", "M", "M", "F", "F", "F", "M", "M", "M"],
    "actual": [10, 20, 30, 15, 25, 35, 12, 22, 32],
    "model_a": [8, 22, 28, 18, 24, 38, None, 19, 35],
    "model_b": [12, None, None, 13, 27, 33, 14, 25, 29],
})

data
```

# Simple Metrics
```{python}
# Define metrics
metrics = [
    MetricDefine(name="mae", label="Mean Absolute Error"),
    MetricDefine(name="rmse", label="Root Mean Squared Error"),
]

# Create evaluator and run evaluation
evaluator = MetricEvaluator(
    df=data,
    metrics=metrics,
    ground_truth="actual",
    estimates=["model_a", "model_b"],
)

evaluator.evaluate()
```

# Subject-level Metrics

## Within Subject Metrics

```{python}
within_subject_metrics = MetricDefine(
        name="mae_per_subject",
        type="within_subject",
        within_expr=pl.col("absolute_error").mean().alias("value"),
        label="MAE per Subject"
    )


evaluator = MetricEvaluator(
    df=data,
    metrics=within_subject_metrics,
    ground_truth="actual",
    estimates=["model_a", "model_b"],
)

evaluator.evaluate()
```


## Across subject metrics

```{python}
across_subject_metrics = MetricDefine(
        name="mae:mean", 
        type="across_subject",
        label="Mean of Subject MAEs"
    )

evaluator = MetricEvaluator(
    df=data,
    metrics=across_subject_metrics,
    ground_truth="actual",
    estimates=["model_a", "model_b"],
)

evaluator.evaluate()
```

## Combining within and between subject metrics 

```{python}
# Combine the metric lists using list concatenation or unpacking
evaluator = MetricEvaluator(
    df=data,
    metrics=[
        within_subject_metrics,
        across_subject_metrics
    ],
    ground_truth="actual",
    estimates=["model_a", "model_b"],
)

evaluator.evaluate()
```

# Visit-level Metrics

## Within Visit Metrics

```{python}
within_visit_metrics = MetricDefine(
        name="mae_per_visit",
        type="within_visit",
        within_expr=pl.col("absolute_error").mean().alias("value"),
        label="MAE per Visit"
    )

evaluator = MetricEvaluator(
    df=data,
    metrics=within_visit_metrics,
    ground_truth="actual",
    estimates=["model_a", "model_b"],
)

evaluator.evaluate()
```

## Across Visit Metrics

```{python}
across_visit_metrics = MetricDefine(
        name="mae_across_visits", 
        type=MetricType.ACROSS_VISIT,
        within_expr=pl.col("absolute_error").mean().alias("value"),  # Per visit first
        across_expr=pl.col("value").mean(),  # Then mean across visits
        label="Mean of Visit MAEs"
    )

evaluator = MetricEvaluator(
    df=data,
    metrics=across_visit_metrics,
    ground_truth="actual",
    estimates=["model_a", "model_b"],
)

evaluator.evaluate()
```

## Combining within and between visit metrics 

```{python}
evaluator = MetricEvaluator(
    df=data,
    metrics=[
        within_visit_metrics,
        across_visit_metrics,
    ],
    ground_truth="actual",
    estimates=["model_a", "model_b"],
)

evaluator.evaluate()
```

# Scope 

## Global Scope 

Global scope metrics compute a single value across the entire dataset, ignoring model and group distinctions.

```{python}
global_scope_metrics = MetricDefine(
    name = "n_subject", 
    scope = "global"
)

evaluator = MetricEvaluator(
    df=data,
    metrics=[
        global_scope_metrics,
    ],
    ground_truth="actual",
    estimates=["model_a", "model_b"],
)

evaluator.evaluate()
```

## Group Scope

Group scope metrics compute one value per group, aggregating across all models.

```{python}
group_scope_metrics = MetricDefine(
    name = "n_subject", 
    scope = "group"
)

evaluator = MetricEvaluator(
    df=data,
    metrics=[group_scope_metrics],
    ground_truth="actual",
    estimates=["model_a", "model_b"],
    group_by=["treatment"],
)

evaluator.evaluate()
```

## Model Scope

Model scope metrics compute one value per model, ignoring group distinctions.

```{python}
model_scope_metrics = MetricDefine(
    name = "n_sample_with_data", 
    scope = "model"
)

evaluator = MetricEvaluator(
    df=data,
    metrics=[model_scope_metrics],
    ground_truth="actual",
    estimates=["model_a", "model_b"],
    group_by=["treatment"],
)

evaluator.evaluate()
```

## Grouping and Stratification

### Group-By Analysis

```{python}
# Evaluate metrics by treatment group
evaluator = MetricEvaluator(
    df=data,
    metrics=[MetricDefine(name="mae"), MetricDefine(name="rmse")],
    ground_truth="actual",
    estimates=["model_a", "model_b"],
    group_by=["treatment"],
)

evaluator.evaluate()
```

### Subgroup Analysis

```{python}
# Evaluate with subgroup stratification (age_group and sex already in data)
evaluator = MetricEvaluator(
    df=data,
    metrics=[MetricDefine(name="mae")],
    ground_truth="actual",
    estimates=["model_a"],
    group_by=["treatment"],
    subgroup_by=["age_group", "sex"],
)

evaluator.evaluate()
```
