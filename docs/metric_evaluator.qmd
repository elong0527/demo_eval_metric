---
title: "MetricEvaluator: Evaluation Pipeline"
---

# MetricEvaluator

The `MetricEvaluator` class is the main evaluation engine that executes metric computations on your data. 
It handles hierarchical aggregations, grouping, filtering, and manages the entire evaluation pipeline using Polars' lazy evaluation for optimal performance.

`MetricEvaluator.evaluate()` returns a Polars `DataFrame` by default. Set
`verbose=True` to include struct columns and diagnostic fields, or
`collect=False` to keep the lazy representation for additional pipeline work.

## Evaluator Inputs

| Argument | Type | Notes |
| --- | --- | --- |
| `df` | `pl.DataFrame | pl.LazyFrame` | Source data. The evaluator keeps it lazy internally. |
| `metrics` | `MetricDefine | list[MetricDefine]` | Metric definitions to execute. Use lists to mix different aggregation types. |
| `ground_truth` | `str` | Column containing observed values. Defaults to `"actual"`. |
| `estimates` | `str | list[str] | dict[str, str]` | Model predictions to compare against `ground_truth`. Dict form lets you control display labels. |
| `group_by` | `list[str] | dict[str, str] | None` | Optional columns for cohort-level summaries (e.g., treatment, site). |
| `subgroup_by` | `list[str] | dict[str, str] | None` | Optional stratifiers that fan out into subgroup-specific rows. |
| `scope` (per metric) | `MetricScope | None` | Overrides default grouping for a metric (`global`, `model`, `group`). |
| `filter_expr` | `pl.Expr | None` | Optional Polars filter applied once up front. |
| `error_params` | `dict[str, dict[str, Any]] | None` | Overrides for error expressions registered in `MetricRegistry`. |

Throughout this page we reuse the synthetic dataset produced by
`generate_sample_data` so the examples stay reproducible.

## Example Data

```{python}
import polars as pl
from polars_eval_metrics import MetricDefine, MetricEvaluator, MetricRegistry
from polars_eval_metrics.metric_registry import MetricInfo
from data_generator import generate_sample_data

# Create sample data using shared generator
data = generate_sample_data(n_subjects=6, n_visits=3, n_groups=2)

data
```

## Quick Start

### Basic evaluation

```{python}
# Define metrics
metrics = [
    MetricDefine(name="mae", label="Mean Absolute Error"),
    MetricDefine(name="rmse", label="Root Mean Squared Error"),
]

# Create evaluator and run evaluation
evaluator = MetricEvaluator(
    df=data,
    metrics=metrics,
    ground_truth="actual",
    estimates=["model1", "model2"],
)

basic_res = evaluator.evaluate()
basic_res
```

`basic_res` is a Polars `DataFrame`. The compact view keeps the core summary
columns (`metric`, `estimate`, `value`, and any group labels) while hiding struct
payloads and diagnostic fields for readability. Use the options below when you
need alternate representations:

```{python}
# Materialise a verbose view (struct + diagnostic columns)
basic_verbose = evaluator.evaluate(verbose=True)
basic_verbose.columns
```

Need to stay lazy? Pass `collect=False` to obtain a `LazyFrame` for further
composition before materialising the result:

```{python}
basic_lazy = evaluator.evaluate(collect=False)

from polars_eval_metrics.ard import ARD

# Wrap the lazy output in the ARD helper when you need canonical struct columns
ard_view = ARD(basic_lazy)
ard_view.collect().head()
```

### Adding groups and scopes

```{python}
# Define metrics
metrics = [
    MetricDefine(name="n_subject", label = "Number of Subjects", scope = "global"), 
    MetricDefine(name="n_sample", label = "Number of Samples", scope = "group"), 
    MetricDefine(name="pct_sample_with_data", label = "Percent of Samples with Data", scope = "group"),
    MetricDefine(name="mae", label="MAE"),
    MetricDefine(name="rmse", label="RMSE"),
]

# Create evaluator and run evaluation
evaluator = MetricEvaluator(
    df=data,
    metrics=metrics,
    ground_truth="actual",
    estimates=["model1", "model2"],
    group_by=["treatment"]
)

res = evaluator.evaluate()
res
```

The evaluation output keeps a lightweight `value` column for quick inspection, but the
full detail lives in the `stat` struct and the companion `stat_fmt`, `warning`, and
`error` columns. Use these when you need typed payloads or diagnostics.

When a `MetricInfo` declares `value_kind="int"`, the evaluator stores the integer
value under `stat.value_int`, ready for reuse in downstream calculations:

```{python}
# Grab integer counts for the subject metrics
res_verbose = evaluator.evaluate(verbose=True)
res_verbose.filter(pl.col("metric") == "n_subject").with_columns(
    pl.col("stat").struct.field("value_int").alias("subject_count")
)
```

### Structured payloads and custom formatting

When a metric returns more than a single scalar, surface it as a struct and
optionally supply a formatter. The evaluator keeps the struct in
`stat.value_struct` while the formatter drives `stat_fmt`:

```{python}
# Register a metric that surfaces a richer payload as a struct
MetricRegistry.register_metric(
    "mae_with_bounds",
    MetricInfo(
        expr=pl.struct(
            [
                pl.col("absolute_error").mean().alias("mean"),
                pl.col("absolute_error").std().alias("sd"),
            ]
        ),
        format="{0[mean]:.1f} +/- {0[sd]:.1f}",
    ),
)

evaluator = MetricEvaluator(
    df=data,
    metrics=MetricDefine(name="mae_with_bounds"),
    ground_truth="actual",
    estimates=["model1"],
)

bounds_res = evaluator.evaluate(verbose=True)
bounds_res.select(["metric", "estimate", "stat_fmt"]).head()

# Inspect the struct payload when needed
bounds_res.select(["metric", "stat"]).head()
```

### Pivot helpers

Both `pivot_by_group()` and `pivot_by_model()` reshape the evaluation output
into presentation-friendly tables while keeping formatted columns intact:

```{python}
evaluator.pivot_by_group()
```

```{python}
evaluator.pivot_by_model()
```

## Subject-level metrics

Subject-oriented aggregations either keep identifiers for every subject (`within_subject`)
or summarise subject-level results into a single row (`across_subject`). The evaluator
handles the hierarchical grouping and preserves entity identifiers in the `id` struct.

### Within-subject metrics

```{python}
within_subject_metrics = MetricDefine(
        name="mae",
        type="within_subject",
        label="MAE per Subject"
    )


evaluator = MetricEvaluator(
    df=data,
    metrics=within_subject_metrics,
    ground_truth="actual",
    estimates=["model1", "model2"],
)

evaluator.evaluate()
```

The resulting rows include an `id` struct with the subject identifiers, which makes
it easy to join back to other subject-level metadata:

```{python}
# Inspect subject identifiers carried in the id struct
evaluator.evaluate().unnest(["id"])
```


### Across-subject metrics

```{python}
across_subject_metrics = MetricDefine(
        name="mae:mean", 
        type="across_subject",
        label="Mean of Subject MAEs"
    )

evaluator = MetricEvaluator(
    df=data,
    metrics=across_subject_metrics,
    ground_truth="actual",
    estimates=["model1", "model2"],
)

evaluator.evaluate()
```

### Combining subject-level views

```{python}
# Combine the metric lists using list concatenation or unpacking
evaluator = MetricEvaluator(
    df=data,
    metrics=[
        within_subject_metrics,
        across_subject_metrics
    ],
    ground_truth="actual",
    estimates=["model1", "model2"],
)

evaluator.evaluate()
```

## Visit-level metrics

Visit metrics mirror the subject patterns but operate on combined `subject_id` / `visit_id`
keys. Use `within_visit` to keep per-visit rows and `across_visit` to summarize the visit
distribution.

### Within-visit metrics

```{python}
within_visit_metrics = MetricDefine(
        name="mae",
        type="within_visit",
        label="MAE per Visit"
    )

evaluator = MetricEvaluator(
    df=data,
    metrics=within_visit_metrics,
    ground_truth="actual",
    estimates=["model1", "model2"],
)

evaluator.evaluate()
```

### Across-visit metrics

```{python}
across_visit_metrics = MetricDefine(
        name="mae:mean", 
        type="across_visit",
        label="Mean of Visit MAEs"
    )

evaluator = MetricEvaluator(
    df=data,
    metrics=across_visit_metrics,
    ground_truth="actual",
    estimates=["model1", "model2"],
)

evaluator.evaluate()
```

### Combining visit-level views

```{python}
evaluator = MetricEvaluator(
    df=data,
    metrics=[
        within_visit_metrics,
        across_visit_metrics,
    ],
    ground_truth="actual",
    estimates=["model1", "model2"],
)

evaluator.evaluate()
```

## Metric scopes

Scopes override the default behaviour that evaluates every metric per estimate and group.
`global` collapses everything into a single row, `group` keeps one row per group value, and
`model` isolates each estimate regardless of group columns.

### Global scope

Global scope metrics compute a single value across the entire dataset, ignoring model and group distinctions.

```{python}
global_scope_metrics = MetricDefine(
    name = "n_subject", 
    scope = "global"
)

evaluator = MetricEvaluator(
    df=data,
    metrics=[
        global_scope_metrics,
    ],
    ground_truth="actual",
    estimates=["model1", "model2"],
)

evaluator.evaluate()
```

### Group scope

Group scope metrics compute one value per group, aggregating across all models.

```{python}
group_scope_metrics = MetricDefine(
    name = "n_subject", 
    scope = "group"
)

evaluator = MetricEvaluator(
    df=data,
    metrics=[group_scope_metrics],
    ground_truth="actual",
    estimates=["model1", "model2"],
    group_by=["treatment"],
)

evaluator.evaluate()
```

### Model scope

Model scope metrics compute one value per model, ignoring group distinctions.

```{python}
model_scope_metrics = MetricDefine(
    name = "n_sample_with_data", 
    scope = "model"
)

evaluator = MetricEvaluator(
    df=data,
    metrics=[model_scope_metrics],
    ground_truth="actual",
    estimates=["model1", "model2"],
    group_by=["treatment"],
)

evaluator.evaluate()
```

## Grouping and stratification

Use `group_by` to partition metrics into cohorts (for example, by treatment arm) and
`subgroup_by` to explode each cohort into additional breakdowns such as gender or race.
Both arguments accept either a list of column names or a mapping to display labels.

### Group-by analysis

```{python}
# Evaluate metrics by treatment group
evaluator = MetricEvaluator(
    df=data,
    metrics=[MetricDefine(name="mae"), MetricDefine(name="rmse")],
    ground_truth="actual",
    estimates=["model1", "model2"],
    group_by=["treatment"],
)

evaluator.evaluate()
```

### Subgroup analysis

```{python}
# Evaluate with subgroup stratification (gender and race already in data)
evaluator = MetricEvaluator(
    df=data,
    metrics=[MetricDefine(name="mae")],
    ground_truth="actual",
    estimates=["model1"],
    group_by=["treatment"],
    subgroup_by=["gender", "race"],
)

evaluator.evaluate()
```

## Filtering and reuse

Large evaluations can be expensive. Two conveniences help keep things fast:

- `evaluate(metrics=..., estimates=...)` lets you rerun the evaluator on a subset of the
  originally configured metrics or estimates without rebuilding the instance.
- `filter(metrics=..., estimates=...)` returns a lightweight evaluator that shares the same
  lazy frame and cached error columns.

Results are cached by `(metric, estimate)` combination, so repeating the same call avoids
recomputation. Call `clear_cache()` when the underlying data changes or you want a fresh
evaluation.

```{python}
# Re-evaluate only MAE for model1 using the cached pipeline
evaluator.evaluate(metrics=MetricDefine(name="mae"), estimates="model1")
```

```{python}
# Or create a filtered evaluator for model2-only summaries
model2_eval = evaluator.filter(estimates="model1")
model2_eval.evaluate()
```
