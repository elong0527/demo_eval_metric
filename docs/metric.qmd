---
title: "MetricDefine"
format: html
execute:
  echo: true
  warning: false
---

# MetricDefine

`MetricDefine` is the core class for defining metrics in the polars-eval-metrics framework. This guide shows you how to define both simple and complex metrics for model evaluation.

## Setup

```{python}
import sys
sys.path.append('../src')

import polars as pl 

from polars_eval_metrics.core.metric_define import MetricDefine
```

## Getting Started: A Simple MAE Metric

Let's start with the simplest possible metric definition - Mean Absolute Error (MAE). This example demonstrates how to define a metric using Polars expressions for lazy evaluation.

```{python}
MetricDefine(name="mae")
```

### Understanding the Output

1. **Basic Information**:
   - `name='mae'`: The metric identifier used to reference this metric
   - `type=across_samples`: The default aggregation type that computes across all samples
   - `Label: 'mae'`: The display label (automatically derived from the name)

2. **Selection Expression**:
   - `[mae] col("absolute_error").mean().alias("value")`: The actual Polars expression
   - `[mae]` indicates this uses the built-in MAE formula
   - The expression calculates the mean of the `absolute_error` column

3. **Polars LazyFrame Chain**:
   - Shows the exact operations that will be executed
   - For `across_samples` metrics, it's a simple `.select()` operation
   - This enables efficient lazy evaluation

## Available Built-in Metrics

The framework provides many common metrics out of the box. Here's the complete list:

> please note the built-in expression will be stored into a `value` column by default. 

```{python}
#| code-fold: true

from polars_eval_metrics import MetricRegistry

pl.Config.set_fmt_str_lengths(200)

# Create a Polars DataFrame of built-in metrics
metrics_data = [
    {"name": name, "expression": str(MetricRegistry.get_metric(name))} 
    for name in sorted(MetricRegistry.list_metrics())
]
metrics_df = pl.DataFrame(metrics_data)
metrics_df
```

## Hierarchical Aggregation

Sometimes you need to calculate metrics at multiple levels. For example, you might want to:
1. Calculate MAE for each subject individually
2. Then take the average across all subjects

This two-level aggregation respects the hierarchical structure of your data and can provide different insights than calculating metrics across all samples directly.

### Using the Colon Convention

For built-in metrics, you can specify two-level aggregation using the colon (`:`) convention:
- Format: `"metric_name:selector_name"`
- Example: `"mae:mean"` calculates MAE per subject, then takes the mean

### Example: Subject-Level MAE

```{python}
# Calculate MAE within each subject, then average across subjects
MetricDefine(name="mae:mean", type="across_subject")
```

Notice how the output now shows both aggregation levels:

1. **Aggregation Expression** `[mae] col("absolute_error").mean().alias("value")`:
   - **First level**: Calculates MAE within each subject group
   - The `[mae]` tag shows this uses the built-in MAE formula

2. **Selection Expression** `[mean] col("value").mean()`:
   - **Second level**: Averages the per-subject MAE values
   - The `[mean]` tag shows this uses the built-in mean selector

3. **Polars LazyFrame Chain**:

```python
.group_by('subject_id')      # Step 1: Group data by subject
.agg(...)                    # Step 2: Calculate for each subject
.select(...)                 # Step 3: Average across all subjects
```

This two-level pattern is particularly useful in clinical trials, longitudinal studies, or any analysis where you need to respect the hierarchical structure of your data.

## Built-in Selectors

Selectors are used in the second level of hierarchical aggregation. They define how to combine the first-level results:

```{python}
#| code-fold: true

# Create a Polars DataFrame of built-in selectors
selectors_data = [
    {"name": name, "expression": str(MetricRegistry.get_selector(name))} 
    for name in sorted(MetricRegistry.list_selectors())
]
selectors_df = pl.DataFrame(selectors_data)
selectors_df
```

## Custom Expressions

Beyond built-in metrics, you can define custom metrics using Polars expressions. 
It provides flexibility to customized functions.

### Percentage of Accurate Predictions

Let's create a custom metric that calculates the percentage of samples where the absolute error is less than 1:

```{python}
MetricDefine(
    name="pct_within_1",
    label="% Predictions Within +/- 1",
    type="across_samples",
    select_expr=(pl.col('absolute_error') < 1).mean() * 100
)
```

### Percentile of per Subject MAE

- calculate MAE for each subject. 
- calculate 90th percentile of the MAE.

```{python}
MetricDefine(
    name="mae_p90_by_subject",
    label="90th Percentile of Subject MAEs",
    type="across_subject",
    agg_expr="mae",
    select_expr=pl.col('value').quantile(0.9)
)
```

In this example we mixed built-in metrics `mae` and 
customized expression. 

### Weighted Average of per Subject MAE

For calculating weighted averages across subjects, we need to:
1. Calculate MAE for each subject
2. Get the weight for each subject (e.g., sample count or custom weight)
3. Calculate the weighted average across subjects

#### Method 1: 

```{python}
MetricDefine(
    name="weighted_mae",
    label="Weighted Average of Subject MAEs",
    type="across_subject",
    agg_expr=[
        "mae",  # MAE per subject (stored in 'value' column)
        pl.col("weight").mean().alias("avg_weight")  # Average weight per subject
    ],
    select_expr=(
        (pl.col('value') * pl.col('avg_weight')).sum() / 
        pl.col('avg_weight').sum()
    )
)
```

#### Method 2: Using Lambda Function
Alternatively, we can use a user-defined function (UDF) with NumPy for the weighted average calculation:

```{python}
import numpy as np

# Define the weighted average expression
weighted_average = (
    pl.struct(['value', 'avg_weight'])
    .map_batches(
        lambda x: pl.Series([
            np.average(
                x.struct.field('value'), 
                weights=x.struct.field('avg_weight')
            )
        ]),
        return_dtype=pl.Float64
    )
)
    
MetricDefine(
    name="weighted_mae_numpy",
    label="Weighted Average of Subject MAEs (NumPy)",
    type="across_subject",
    agg_expr=[
        "mae",  # MAE per subject (stored in 'value' column)
        pl.col("weight").mean().alias("avg_weight")  # Average weight per subject
    ],
    select_expr=weighted_average
)
```

