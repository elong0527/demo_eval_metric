---
title: "Quick Start"
---

# Quick Start with Polars Eval Metrics

[![codecov](https://codecov.io/gh/elong0527/demo_eval_metric/branch/main/graph/badge.svg)](https://codecov.io/gh/elong0527/demo_eval_metric)
[![Tests](https://github.com/elong0527/demo_eval_metric/actions/workflows/test.yml/badge.svg)](https://github.com/elong0527/demo_eval_metric/actions/workflows/test.yml)

This guide will help you get up and running with the polars-eval-metrics package.

## Setup

First, ensure the package is available in your Python environment:

```{python}
#| echo: false
import sys
import os
sys.path.insert(0, os.path.abspath("../src"))
```

```{python}
# Import the package
import polars as pl
from polars_eval_metrics import (
    MetricDefine,
    MetricFactory,
    MetricEvaluator,
    MetricType,
    EvaluationConfig
)
from polars_eval_metrics.core.metric_define import MetricDefine

print("Package loaded successfully!")
```

## Core Concepts

### 1. Metric Types

The framework supports five types of metric aggregations:

```{python}
# Display available metric types
for metric_type in MetricType:
    print(f"- {metric_type.value}: {metric_type.name}")
```

### 2. Metric Definition

Metrics can be defined in two ways:

#### Programmatically with MetricDefine

```{python}
# Define a metric directly
metric = MetricDefine(
    name="mae",
    label="Mean Absolute Error",
    type=MetricType.ACROSS_SAMPLES
)

print(f"Created metric: {metric.name} ({metric.label})")
```

#### From YAML Configuration with MetricFactory

```{python}
# Define from configuration
config = {
    'name': 'rmse',
    'label': 'Root Mean Squared Error',
    'type': 'across_samples'
}

metric = MetricFactory.from_yaml(config)
print(f"Created metric: {metric.name} ({metric.label})")
```

### 3. Sample Data

Let's generate some sample data to work with:

```{python}
from data_generator import generate_sample_data

# Generate sample data
df = generate_sample_data(n_subjects=3, n_visits=2, n_groups=2)
print(f"Data shape: {df.shape}")
print(f"Columns: {df.columns}")

# Show first few rows
df.head()
```

### 4. Basic Evaluation

Now let's evaluate some metrics:

```{python}
# Define multiple metrics
config = {
    'metrics': [
        {'name': 'mae', 'label': 'MAE'},
        {'name': 'rmse', 'label': 'RMSE'},
        {'name': 'bias', 'label': 'Bias'}
    ]
}

# Create metrics directly from config
metrics = MetricFactory.from_dict(config)

# Create evaluator
evaluator = MetricEvaluator(
    df=df,
    metrics=metrics,
    ground_truth="actual",
    estimates=["model1", "model2"],
    group_by=["treatment"]
)

# Evaluate all metrics
results = evaluator.evaluate_all()
results
```

### 5. Configuration-Based Workflow

You can also use a complete configuration:

```{python}
# Complete configuration
full_config = {
    'ground_truth': 'actual',
    'estimates': ['model1', 'model2'],
    'group_by': ['treatment'],
    'metrics': [
        {'name': 'mae', 'label': 'Mean Absolute Error'},
        {'name': 'mse', 'label': 'Mean Squared Error'}
    ]
}

# Load configuration
eval_config = EvaluationConfig.from_yaml(full_config)

# Create evaluator from config
evaluator = MetricEvaluator(
    df=df,
    **eval_config.to_evaluator_kwargs()
)

# Evaluate
results = evaluator.evaluate_all()
results
```

```{python}
# Pivot for better readability
pivot_results = results.pivot(
    values="value",
    index=["treatment", "metric"],
    on="estimate"
)
pivot_results
```

### 6. Custom Metrics with Type-Safe Expressions

The framework requires actual Polars expressions for custom metrics, providing type safety and IDE support:

```{python}
# Define a custom metric with Polars expression
custom_metric = MetricDefine(
    name="accuracy_90",
    label="90% Accuracy Rate",
    type=MetricType.ACROSS_SAMPLES,
    select_expr=(pl.col('absolute_error') < pl.col('actual').abs() * 0.1).mean() * 100
)

print(f"Custom metric created: {custom_metric.name}")
print(f"Expression type: {type(custom_metric.select_expr)}")
```

Note: String expressions are NOT supported - you must use actual Polars expression objects:

```{python}
#| error: true
# This will raise a validation error
try:
    invalid_metric = MetricDefine(
        name="invalid",
        select_expr="pl.col('error').mean()"  # String not allowed!
    )
except Exception as e:
    print(f"Error: {type(e).__name__}")
```

## Next Steps

- [Metric Examples](metric.qmd) - See detailed examples and use cases
- [Expression Validation](expression_validation.qmd) - Learn about type-safe expressions