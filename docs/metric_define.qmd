---
title: "MetricDefine"
---

# MetricDefine

`MetricDefine` is the entry point for describing metrics in the polars-eval-metrics
framework. A definition bundles the metric identifier, optional labels, the
aggregation pattern, and the Polars expressions that power lazy evaluation. Once
defined, the metric can be registered globally or handed directly to
`MetricEvaluator`.

## Key Parameters

| Argument | Type | Default | Notes |
| --- | --- | --- | --- |
| `name` | `str` | required | Unique identifier. Use the `metric:summary` form (for example `"mae:mean"`) to compose built-ins. |
| `label` | `str | None` | derived from `name` | Optional display name. When omitted the name string is reused. |
| `type` | `MetricType` | `across_sample` | Controls the aggregation level. Determines whether `within_expr` or `across_expr` are required. |
| `scope` | `MetricScope | None` | `None` | Narrows where the metric is applied (`global`, `model`, or `group`). When omitted the evaluator keeps its default grouping over estimates and group columns. |
| `within_expr` | `list[str | pl.Expr | MetricInfo] | None` | `None` | First-level aggregations. Accept built-in metric names, Polars expressions, or `MetricInfo` instances. Required for custom `across_subject` / `across_visit` metrics. |
| `across_expr` | `str | pl.Expr | MetricInfo | None` | `None` | Second-level aggregation or final expression. Strings reference summary names in `MetricRegistry`. |

### Metric Types

- `across_sample`: Aggregates directly from the sample-level errors.
- `within_subject` / `within_visit`: Produces metrics inside each entity and
  keeps identifiers in the result `id` struct.
- `across_subject` / `across_visit`: Uses `within_expr` to summarise inside the
  entity, then `across_expr` for the overall statistic.

### Metric Scopes

`scope` is orthogonal to the aggregation type and controls where a metric is
computed:

- `global`: Run once for the entire dataset.
- `model`: Run per model, ignoring group splits.
- `group`: Run per group, ignoring per-model splits.

If `scope` is omitted the evaluator inherits its default behaviour.

## Setup

```{python}
import polars as pl

pl.Config.set_fmt_str_lengths(200)
pl.Config.set_tbl_rows(-1)

from polars_eval_metrics import MetricDefine
```

## Quick Start: Mean Absolute Error

A minimal definition creates the built-in MAE metric. The evaluator injects the
`absolute_error` column, so only the metric name is required.

```{python}
MetricDefine(name="mae")
```

::: {.callout-note label="What the repr tells you"}
- `name` and `label`: how other components reference the metric.
- `type=across_sample`: MAE operates over all samples without grouping.
- `summary`: the Polars expression backing the metric. Built-ins automatically
  alias the result to `value`.
- `pl chain`: the lazy operations the evaluator applies when executing the
  definition.
:::

## Inspect Built-in Metrics

The registry exposes the available metrics and summaries. The expressions are
returned as Polars lazy operations that fit directly into `within_expr` or
`across_expr`.

> Built-in metrics always populate the `value` column unless you extend the
> registry with a custom `MetricInfo` that carries a different payload.

```{python}
#| code-fold: true

from polars_eval_metrics import MetricRegistry

metrics_data = [
    {"name": name, "expression": str(MetricRegistry.get_metric(name))}
    for name in sorted(MetricRegistry.list_metrics())
]
pl.DataFrame(metrics_data)
```

```{python}
#| code-fold: true

summaries_data = [
    {"name": name, "expression": str(MetricRegistry.get_summary(name))}
    for name in sorted(MetricRegistry.list_summaries())
]
pl.DataFrame(summaries_data)
```

## Hierarchical Aggregation Patterns

Many analyses require metrics at multiple levels (for example subject or visit
summaries). Built-in metric names support the colon convention, so
`"mae:mean"` computes MAE within each subject and then averages the result.

```{python}
MetricDefine(name="mae:mean", type="across_subject")
```

Internally the definition resolves to:

1. `[mae] col("absolute_error").mean()` — per-subject MAE.
2. `[mean] col("value").mean()` — average of the per-subject values.
3. Polars lazy chain equivalent to:
   ```python
   .group_by("subject_id").agg(...).select(...)
   ```

Apply the same `metric:summary` pattern to visits or any other second-level
summary that lives in the registry.

## Custom Expressions

To go beyond the built-ins, supply Polars expressions. Strings in
`within_expr` or `across_expr` still resolve through the registry, while
expressions give you full control over column math.

```{python}
MetricDefine(
    name="pct_within_1",
    label="% Predictions Within +/- 1",
    type="across_sample",
    across_expr=(pl.col("absolute_error") < 1).mean() * 100,
)
```

When a `within_expr` list returns more than one expression, alias each output so
you can reference it from `across_expr`:

```{python}
MetricDefine(
    name="mae_p90_by_subject",
    label="90th Percentile of Subject MAEs",
    type="across_subject",
    within_expr="mae",  # resolves to the built-in metric, stored in `value`
    across_expr=pl.col("value").quantile(0.9, interpolation="linear"),
)
```

### Mixing Built-ins and Custom Outputs

Combining built-ins with additional columns lets you calculate more advanced
statistics, such as a weighted average that includes per-subject weights.

```{python}
MetricDefine(
    name="weighted_mae",
    label="Weighted Average of Subject MAEs",
    type="across_subject",
    within_expr=[
        "mae",  # stored as `value`
        pl.col("weight").mean().alias("avg_weight"),
    ],
    across_expr=(
        (pl.col("value") * pl.col("avg_weight")).sum()
        / pl.col("avg_weight").sum()
    ),
)
```

### Inline struct output: `mean +/- sd`

You can return richer payloads without registering the metric globally by
passing a `MetricInfo` directly to `MetricDefine`. The evaluator inspects the
expression output to determine the stored type, while the optional `format`
string controls how `stat_fmt` renders the value:

```{python}
from polars_eval_metrics.metric_registry import MetricInfo

MetricDefine(
    name="mean_sd_inline",
    type="across_sample",
    across_expr=MetricInfo(
        expr=pl.struct([
            pl.col("absolute_error").mean().alias("mean"),
            pl.col("absolute_error").std().alias("sd"),
        ]),
        format="{0[mean]:.1f} +/- {0[sd]:.1f}",
    ),
)
```

The resulting metric stores the struct in `stat.value_struct` while
`stat_fmt` (and the `value` column when formatted) displays the familiar
`mean ± sd` string.

## Evaluate with MetricEvaluator

`MetricDefine` instances plug directly into `MetricEvaluator`. The evaluator
feeds error columns, applies scopes, and materialises results as lazy frames.

```{python}
from polars_eval_metrics import MetricEvaluator
from data_generator import generate_sample_data

data = generate_sample_data(n_subjects=6, n_visits=3, n_groups=2)

metrics = [
    MetricDefine(name="mae"),
    MetricDefine(
        name="pct_within_1",
        type="across_sample",
        across_expr=(pl.col("absolute_error") < 1).mean() * 100,
    ),
    MetricDefine(name="mae:mean", type="across_subject"),
]

evaluator = MetricEvaluator(
    df=data,
    metrics=metrics,
    ground_truth="actual",
    estimates=["model1", "model2"],
    group_by=["treatment"],
)

evaluator.evaluate()
```

The resulting frame keeps a formatted `value` column plus a `stat` struct that
preserves typed payloads. When you register metrics with
`MetricRegistry.register_metric`, use `MetricInfo(value_kind=...)` to surface
non-float structures that downstream consumers can unwrap.

## Advanced: Custom Functions

You can opt into user-defined functions when a vectorised Polars expression is
not available. The following example keeps the earlier weighted MAE structure
but performs the across-entity step inside a NumPy call.

```{python}
import numpy as np

weighted_average = (
    pl.struct(["value", "avg_weight"]).map_batches(
        lambda rows: pl.Series(
            [
                np.average(
                    rows.struct.field("value"),
                    weights=rows.struct.field("avg_weight"),
                )
            ]
        ),
        return_dtype=pl.Float64,
    )
)

MetricDefine(
    name="weighted_mae_numpy",
    label="Weighted Average of Subject MAEs (NumPy)",
    type="across_subject",
    within_expr=[
        "mae",
        pl.col("weight").mean().alias("avg_weight"),
    ],
    across_expr=weighted_average,
)
```

::: {.callout-warning}
Batch UDFs disable some Polars optimisations and can be slower than pure
expressions. Prefer native expressions when possible, and reserve UDFs for
cases where vectorised operations do not exist.
:::
