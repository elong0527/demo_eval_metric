---
title: "Polars Eval Metrics"
---

# Welcome to Polars Eval Metrics

[![codecov](https://codecov.io/gh/elong0527/demo_eval_metric/branch/main/graph/badge.svg)](https://codecov.io/gh/elong0527/demo_eval_metric)
[![Tests](https://github.com/elong0527/demo_eval_metric/actions/workflows/test.yml/badge.svg)](https://github.com/elong0527/demo_eval_metric/actions/workflows/test.yml)
[![Documentation](https://github.com/elong0527/demo_eval_metric/actions/workflows/docs.yml/badge.svg)](https://github.com/elong0527/demo_eval_metric/actions/workflows/docs.yml)

A high-performance model evaluation framework built on Polars lazy evaluation.

## Features

- **Fast**: Leverages Polars lazy evaluation for optimal performance
- **Flexible**: Support for custom metrics and expressions
- **Type-safe**: Pydantic models with validation
- **Simple**: Clean API with sensible defaults
- **Extensible**: Easy to add new metrics and aggregation types

## Quick Start

```python
from polars_eval_metrics import MetricEvaluator, MetricFactory

# Define metrics from configuration
config = {
    'metrics': [
        {'name': 'mae', 'label': 'Mean Absolute Error'},
        {'name': 'rmse', 'label': 'Root Mean Squared Error'}
    ]
}

# Create metrics directly from config
metrics = MetricFactory.from_dict(config)

# Evaluate
evaluator = MetricEvaluator(
    df=data,
    metrics=metrics,
    ground_truth="actual",
    estimates=["model1", "model2"],
    group_by=["treatment"]
)

results = evaluator.evaluate_all()
```

## Installation

To use this package, make sure it's in your Python path:

```bash
# If running from the repository
export PYTHONPATH=/path/to/repo/src:$PYTHONPATH

# Or install in development mode
pip install -e .
```

## Documentation

- [Quick Start](quickstart.qmd) - Installation and basic concepts
- [Metric Examples](metric.qmd) - Detailed examples and use cases