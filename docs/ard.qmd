---
title: "Analysis Results Data (ARD)"
subtitle: "Unified data structure for evaluation results"
---

# Introduction to ARD

The Analysis Results Data (ARD) structure is a standardized format for storing and manipulating evaluation results. It provides a consistent, type-safe schema that supports complex hierarchical data while maintaining high performance through Polars lazy evaluation.

## Why ARD?

Traditional approaches to storing evaluation results often use ad-hoc schemas with varying column names and structures. ARD solves this by providing:

- **Fixed Schema**: Always 7 columns, regardless of complexity
- **Type Safety**: Structured data with validation
- **Flexibility**: Handles any combination of groups, metrics, and estimates
- **Performance**: Built on Polars lazy evaluation
- **Interoperability**: Easy conversion to wide/long formats

## Core Schema

ARD uses exactly 7 columns in every dataset:

| Column | Type | Description |
|--------|------|-------------|
| `groups` | `pl.Struct` | Group variables and values (null if no groups) |
| `subgroups` | `pl.Struct` | Subgroup variables and values (null if no subgroups) |
| `estimate` | `pl.Utf8` | Model/estimate identifier (null if not applicable) |
| `metric` | `pl.Utf8` | Metric identifier |
| `stat` | `pl.Struct` | Statistical value with type information |
| `context` | `pl.Struct` | Metadata and computation details |
| `id` | `pl.Struct` | Optional entity identifiers (subject, visit, etc.) |

The `stat` struct contains:
- `type`: Data type hint ("int", "float", "string", "struct" â€¦)
- `value_float`, `value_int`, `value_bool`, `value_str`, `value_struct`: Typed value channels
- `format`: Optional format string for display
- `unit`: Optional unit of measurement
- `extras`: Optional structured payload for metric-specific attachments

The `id` struct is populated for within-entity metrics such as per-subject or per-visit summaries. It mirrors whatever identifier columns were present in the evaluation pipeline and remains `null` for global metrics.

# Getting Started

```{python}
import sys
import os
sys.path.insert(0, os.path.abspath("../src"))

import polars as pl
from polars_eval_metrics.ard import ARD

# Create ARD from simple records
records = [
    {
        "groups": {"treatment": "A", "site": "01"},
        "subgroups": None,
        "estimate": "model1",
        "metric": "mae",
        "stat": 3.2,
        "context": {"n_obs": 100}
    },
    {
        "groups": {"treatment": "B", "site": "01"},
        "subgroups": {"gender": "Female"},
        "estimate": "model1",
        "metric": "mae",
        "stat": 2.8,
        "context": {"n_obs": 85}
    },
    {
        "groups": {"treatment": "A", "site": "02"},
        "subgroups": None,
        "estimate": "model2",
        "metric": "rmse",
        "stat": 4.1,
        "context": {"n_obs": 120}
    }
]

ard = ARD(records)
print(ard)
```

The ARD automatically handles different data types and normalizes them into the structured format:

```{python}
# Check the underlying data structure
df = ard.collect()
print(f"Shape: {df.shape}")
print(f"Columns: {df.columns}")
print(f"\nFirst stat structure:")
print(df["stat"][0])
```

# Working with Different Stat Types

ARD automatically handles various statistical value types:

```{python}
# Different stat types
diverse_records = [
    {"metric": "count", "stat": 42},  # Integer
    {"metric": "mean", "stat": 3.14159},  # Float
    {"metric": "status", "stat": "significant"},  # String
    {"metric": "ci_95", "stat": {"lower": 2.1, "upper": 4.8}},  # Complex struct
    {"metric": "formatted", "stat": {"value": 0.123, "type": "float", "format": "{:.2%}", "unit": "accuracy"}}  # With metadata
]

stats_ard = ARD(diverse_records)
print(stats_ard)
```

```{python}
# Extract just the values
values_df = stats_ard.get_stats()
print("Values only:")
print(values_df)

print("\nWith metadata:")
meta_df = stats_ard.get_stats(include_metadata=True)
print(meta_df)
```

# Filtering and Querying

ARD exposes its data as a Polars `LazyFrame`, so you can compose filters with
standard Polars expressions:

```{python}
# Create a larger dataset for filtering examples
large_records = []
for treatment in ["A", "B", "C"]:
    for site in ["01", "02", "03"]:
        for model in ["model1", "model2"]:
            for metric in ["mae", "rmse", "r2"]:
                large_records.append({
                    "groups": {"treatment": treatment, "site": site},
                    "estimate": model,
                    "metric": metric,
                    "stat": 2.5 + hash(f"{treatment}{site}{model}{metric}") % 100 / 50.0  # Fake data
                })

large_ard = ARD(large_records)
print(f"Total records: {len(large_ard)}")
```

## Filter by Groups

```{python}
# Filter by specific group values using Polars expressions
treatment_a = ARD(
    large_ard.lazy.filter(pl.col("groups").struct.field("treatment") == "A")
)
print(f"Treatment A only: {len(treatment_a)} records")

# Multiple group filters
site_01_treatment_b = ARD(
    large_ard.lazy.filter(
        (pl.col("groups").struct.field("treatment") == "B")
        & (pl.col("groups").struct.field("site") == "01")
    )
)
print(f"Treatment B, Site 01: {len(site_01_treatment_b)} records")
print(site_01_treatment_b.collect())
```

## Filter by Metrics and Estimates

```{python}
# Filter by metrics
mae_only = ARD(large_ard.lazy.filter(pl.col("metric") == "mae"))
print(f"MAE only: {len(mae_only)} records")

# Multiple metrics
accuracy_metrics = ARD(
    large_ard.lazy.filter(pl.col("metric").is_in(["mae", "r2"]))
)
print(f"MAE and R2: {len(accuracy_metrics)} records")

# Filter by estimates
model1_only = ARD(large_ard.lazy.filter(pl.col("estimate") == "model1"))
print(f"Model1 only: {len(model1_only)} records")

# Combine filters
filtered = ARD(
    large_ard.lazy.filter(
        (pl.col("groups").struct.field("treatment") == "A")
        & pl.col("metric").is_in(["mae", "rmse"])
        & (pl.col("estimate") == "model1")
    )
)
print(f"Combined filter: {len(filtered)} records")
print(filtered.collect())
```

# Data Transformations

## Converting to Wide Format

ARD can easily pivot to wide format for analysis:

```{python}
# Create sample data for pivoting
pivot_records = [
    {"groups": {"treatment": "A"}, "estimate": "model1", "metric": "mae", "stat": 3.2},
    {"groups": {"treatment": "A"}, "estimate": "model1", "metric": "rmse", "stat": 4.1},
    {"groups": {"treatment": "A"}, "estimate": "model2", "metric": "mae", "stat": 3.0},
    {"groups": {"treatment": "A"}, "estimate": "model2", "metric": "rmse", "stat": 3.8},
    {"groups": {"treatment": "B"}, "estimate": "model1", "metric": "mae", "stat": 2.8},
    {"groups": {"treatment": "B"}, "estimate": "model1", "metric": "rmse", "stat": 3.5},
    {"groups": {"treatment": "B"}, "estimate": "model2", "metric": "mae", "stat": 2.6},
    {"groups": {"treatment": "B"}, "estimate": "model2", "metric": "rmse", "stat": 3.2},
]

pivot_ard = ARD(pivot_records)
print("Original long format:")
print(pivot_ard)
```

```{python}
# Convert to wide format
wide_df = pivot_ard.to_wide()
print("\nWide format:")
print(wide_df)
```

```{python}
# Custom pivot - just by metrics
metrics_wide = pivot_ard.to_wide(columns=["metric"])
print("Pivot by metrics only:")
print(metrics_wide)
```

## Unnesting Structures

```{python}
# Unnest group and subgroup columns
unnested_df = pivot_ard.unnest(["groups"])
print("Unnested groups:")
print(unnested_df.head())
```

## Converting to Long Format

```{python}
# Full normalization to long format
long_df = pivot_ard.to_long()
print("Fully normalized long format:")
print(long_df.head())
```

# Working with Subgroups

ARD handles hierarchical grouping through the subgroups column:

```{python}
# Data with multiple levels of grouping
subgroup_records = [
    {
        "groups": {"study": "STUDY1", "treatment": "A"},
        "subgroups": {"age_group": "18-65", "gender": "Female"},
        "metric": "response_rate",
        "stat": 0.85
    },
    {
        "groups": {"study": "STUDY1", "treatment": "A"},
        "subgroups": {"age_group": "18-65", "gender": "Male"},
        "metric": "response_rate",
        "stat": 0.78
    },
    {
        "groups": {"study": "STUDY1", "treatment": "B"},
        "subgroups": {"age_group": "65+", "gender": "Female"},
        "metric": "response_rate",
        "stat": 0.72
    },
    {
        "groups": {"study": "STUDY1", "treatment": "B"},
        "subgroups": None,  # Overall treatment B
        "metric": "response_rate",
        "stat": 0.75
    }
]

subgroup_ard = ARD(subgroup_records)
print(subgroup_ard)
```

```{python}
# Unnest to see the full structure
full_unnest = subgroup_ard.unnest(["groups", "subgroups"])
print("Fully unnested:")
print(full_unnest)
```

## Summary and Inspection

```{python}
# Use the larger dataset for summary and advanced operations
converted_ard = large_ard
```

```{python}
# Get summary statistics
summary = converted_ard.summary()
print("ARD Summary:")
for key, value in summary.items():
    print(f"  {key}: {value}")
```

```{python}
# Detailed description
converted_ard.describe()
```

## Custom Expressions and Filtering

```{python}
# Use Polars expressions for complex filtering on the long format
import polars as pl

long_values = converted_ard.to_long().with_columns(
    pl.col("stat")
    .map_elements(ARD._stat_value, return_dtype=pl.Float64)
    .alias("value_numeric")
)

custom_filtered = long_values.filter(pl.col("value_numeric") > 3.0)
print(f"Values > 3.0: {custom_filtered.height} records")
print(custom_filtered.select(pl.exclude("value_numeric")))
```

## Handling Missing Data

```{python}
# Create data with missing values
missing_records = [
    {"groups": {"treatment": "A"}, "estimate": "model1", "metric": "mae", "stat": 3.2},
    {"groups": None, "estimate": None, "metric": "mae", "stat": 2.8},  # Missing groups
    {"groups": {"treatment": "B"}, "estimate": "", "metric": "rmse", "stat": 4.1},  # Empty estimate
]

missing_ard = ARD(missing_records)
print("With missing data:")
print(missing_ard)
```

```{python}
# Handle null values
null_to_empty = missing_ard.with_null_as_empty()
print("Null to empty:")
print(null_to_empty)

empty_to_null = missing_ard.with_empty_as_null()
print("Empty to null:")
print(empty_to_null)
```

# Best Practices

## 1. Use ARD for Standardization

Always convert evaluation results to ARD format early in your pipeline for consistent handling.

## 2. Leverage Lazy Evaluation

ARD operations are lazy by default - chain operations before collecting:

```{python}
# Good: Chain operations efficiently using the underlying LazyFrame
result = (
    ARD(
        large_ard.lazy
        .filter(pl.col("groups").struct.field("treatment") == "A")
        .filter(pl.col("metric").is_in(["mae", "rmse"]))
    )
    .to_wide()
)

print(f"Chained operations result: {result.shape}")
```

## 3. Structure Your Data Hierarchically

Use groups for primary analysis dimensions and subgroups for secondary breakdowns:

```{python}
# Well-structured example
structured_records = [
    {
        "groups": {"study": "MAIN", "treatment": "Active"},  # Primary analysis
        "subgroups": {"region": "US", "age_group": "Adult"},  # Secondary breakdowns
        "estimate": "primary_model",
        "metric": "efficacy",
        "stat": {"value": 0.85, "type": "float", "unit": "proportion"}
    }
]

structured_ard = ARD(structured_records)
print("Well-structured ARD:")
print(structured_ard)
```

## 4. Include Context Information

Use the context field for metadata that aids interpretation:

```{python}
# Rich context example
contextual_records = [
    {
        "groups": {"treatment": "A"},
        "metric": "response_rate",
        "stat": {"value": 0.75, "type": "float", "format": "{:.1%}"},
        "context": {
            "n_subjects": 120,
            "n_responders": 90,
            "confidence_level": 0.95,
            "analysis_date": "2024-01-15",
            "data_cutoff": "2024-01-01"
        }
    }
]

contextual_ard = ARD(contextual_records)
ctx_df = contextual_ard.unnest(["context"])
print("With rich context:")
print(ctx_df.head())
```

# Conclusion

ARD provides a robust, flexible foundation for evaluation results that:

- Standardizes data structure across different analyses
- Maintains type safety and validation
- Enables efficient transformations and filtering
- Scales to large datasets through lazy evaluation
- Facilitates easy conversion between formats

By adopting ARD as your standard results format, you gain consistency, performance, and flexibility for all your evaluation workflows.
