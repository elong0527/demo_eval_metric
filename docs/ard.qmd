---
title: "Analysis Results Data (ARD)"
subtitle: "Fixed-schema container for evaluation metrics"
---

```{python}
#| echo: false
import sys
import os
sys.path.insert(0, os.path.abspath("../src"))
```

```{python}
import polars as pl
from polars_eval_metrics import MetricDefine, MetricEvaluator
from polars_eval_metrics.ard import ARD
from data_generator import generate_sample_data

pl.Config.set_tbl_rows(8)
```

# Overview

The Analysis Results Data (ARD) container holds 
every evaluation result produced by `MetricEvaluator`. 
Each ARD instance wraps a Polars `LazyFrame` so results stay lazy
until you explicitly collect them. 
The container standardises the schema for downstream
processing, keeps metric metadata attached, and preserves display-friendly ordering through enum-backed columns.

## Why ARD?

- **Canonical layout**: every result set shares the same structured columns, regardless of
  metric mix or grouping configuration.
- **Type preservation**: statistical values live in a typed struct that keeps floats,
  integers, booleans, strings, and nested payloads distinct.
- **Lazy by default**: filters and joins can be composed on the underlying lazy frame and
  collected only when needed.
- **Display metadata**: columns such as `metric`, `label`, and `estimate` use `Enum` types
  so tables respect the metric definition order instead of alphabetical sorting.

## Canonical Columns

The minimum schema that every ARD provides is shown below. Calling `ARD.schema` exposes
these columns.

| Column | Type | Description |
|--------|------|-------------|
| `groups` | `pl.Struct` | Primary grouping keys (e.g. treatment, site). |
| `subgroups` | `pl.Struct` | Subgroup breakdowns (e.g. gender, race). |
| `estimate` | `pl.Enum` | Estimate identifier for the model or prediction column. |
| `metric` | `pl.Enum` | Metric name registered in the metric registry. |
| `label` | `pl.Utf8`| Human-readable display label |
| `stat` | `pl.Struct` | Typed value container (see below). |
| `stat_fmt` | `pl.Utf8` | Default formatted presentation of the statistic. |
| `context` | `pl.Struct` | Metadata describing how the metric was computed. |
| `warning` | `pl.List(pl.Utf8)` | Captured warnings generated while evaluating the metric. |
| `error` | `pl.List(pl.Utf8)` | Captured errors when evaluation falls back to placeholders. |
| `id` | `pl.Struct` | Entity identifiers for within-subject / visit metrics. |

The `stat` struct splits the stored result across typed channels:

- `type`: value hint such as `"float"`, `"int"`, `"bool"`, `"string"`, or `"struct"`
- `value_float`, `value_int`, `value_bool`, `value_str`, `value_struct`: mutually exclusive
  slots for the actual statistic
- `format`: optional Python format string used when rendering. The rendered value is kept in `stat_fmt` so downstream code can use either the raw struct or the default text representation without recomputing formatting.

Any warnings or errors that occur while evaluating a metric are preserved in the `warning` and `error` list columns. When evaluation succeeds both columns contain empty lists; if an expression fails, the ARD still returns a placeholder row with the captured diagnostic message so the failure is visible without breaking collection.

# Producing ARD from an Evaluation

The evaluator returns a Polars `DataFrame` by default, keeping columns such as
`label`, `metric`, `estimate`, and `value` ready for display. When you need the
full ARD structure, reuse the lazy output with the :class:`polars_eval_metrics.ard.ARD`
helper.

```{python}
# Sample data with two estimates, grouped by treatment and gender
raw = generate_sample_data(n_subjects=4, n_visits=2, n_groups=2)
metrics = [
    MetricDefine(name="mae", label="Mean Absolute Error"),
    MetricDefine(name="rmse", label="Root Mean Squared Error"),
]

evaluator = MetricEvaluator(
    df=raw,
    metrics=metrics,
    ground_truth="actual",
    estimates=["model1", "model2"],
)

result = evaluator.evaluate()
result
```


# ARD Container

Convert the evaluation output to an ARD object:

```{python}
lazy_result = evaluator.evaluate(collect=False)
ard = ARD(lazy_result)
ard
```

```{python}
ard.schema
```

Collecting produces the backward-compatible table with struct columns:

```{python}
canonical = ard.collect()
canonical
```

The typed `stat` payload is preserved even though the `value` column is purely presentational:

```{python}
canonical["stat"][0]
```

Structured columns can be unnested to inspect individual components:

```{python}
canonical.select([
    pl.col("estimate"),
    pl.col("metric"),
    pl.col("stat").struct.field("value_float").alias("value_float"),
    pl.col("stat").struct.field("format").alias("format"),
])
```

`ARD.get_stats()` offers a quick way to access raw values with optional metadata:

Call it without arguments when you just want the canonical metric/value pairs:

```{python}
ard.get_stats()
```

Pass `include_metadata=True` if you need to see the stored `stat` type tag and format hint alongside the value:

```{python}
ard.get_stats(include_metadata=True)
```

# Normalising Struct Columns

Empty structs are often produced when downstream code prefers explicit placeholders. The
helper methods on `ARD` let you toggle between empty and null representations:

`with_empty_as_null()` collapses all-null structs and blank estimates to proper nulls so filters behave as expected:

```{python}
ard.with_empty_as_null().collect()
```

`with_null_as_empty()` does the reverse—replacing nulls with empty structs so templating code can access the fields safely:

```{python}
ard.with_null_as_empty().collect()
```

# Transformation

`to_long()` flattens the lazy frame into a DataFrame where grouping metadata is expanded and the formatted `stat` value is exposed as `value`:

```{python}
ard.to_long()
```

`unnest()` is useful when you just need the struct columns expanded in place without the extra pivoting logic:

```{python}
ard.unnest()
```

Wide presentations remain useful for dashboards. 
`to_wide()` pivots metric values while
preserving formatting hints—perfect for quick scorecards:

```{python}
ard.to_wide(index=["estimate"], columns=["metric"]).head()
```

For ad-hoc layouts you can pull the lazy frame to long form and use Polars' native pivot
via the `pivot()` convenience wrapper when you want full control over the value column and aggregation:

```{python}
ard.pivot(on="metric", index=["estimate"], values="stat")
```

# Summaries

Use `summary()` for a quick diagnostic of the collected dataset or `describe()` for a
formatted printout:

`summary()` returns a dict of counts that you can log or feed into tests:

```{python}
ard.summary()
```

`describe()` emits a readable console report with sample rows—handy during notebook exploration:

```{python}
ard.describe()
```
