---
title: "Analysis Results Data (ARD)"
subtitle: "Fixed-schema container for evaluation metrics"
---

```{python}
#| echo: false
import sys
import os
sys.path.insert(0, os.path.abspath("../src"))
```

```{python}
import polars as pl
from polars_eval_metrics import MetricDefine, MetricEvaluator
from polars_eval_metrics.ard import ARD
from data_generator import generate_sample_data

pl.Config.set_tbl_rows(8)
```

# Overview

The Analysis Results Data (ARD) container holds 
every evaluation result produced by `MetricEvaluator`. 
Each ARD instance wraps a Polars `LazyFrame` so results stay lazy
until you explicitly collect them. 
The container standardises the schema for downstream
processing, keeps metric metadata attached, and preserves display-friendly ordering through enum-backed columns.

## Why ARD?

- **Canonical layout**: every result set shares the same structured columns, regardless of
  metric mix or grouping configuration.
- **Type preservation**: statistical values live in a typed struct that keeps floats,
  integers, booleans, strings, and nested payloads distinct.
- **Lazy by default**: filters and joins can be composed on the underlying lazy frame and
  collected only when needed.
- **Display metadata**: columns such as `metric`, `label`, and `estimate` use `Enum` types
  so tables respect the metric definition order instead of alphabetical sorting.

## Canonical Columns

The minimum schema that every ARD provides is shown below. Calling `ARD.schema` exposes
these columns.

| Column | Type | Description |
|--------|------|-------------|
| `groups` | `pl.Struct` | Primary grouping keys (e.g. treatment, site). |
| `subgroups` | `pl.Struct` | Subgroup breakdowns (e.g. gender, race). |
| `estimate` | `pl.Enum` | Estimate identifier for the model or prediction column. |
| `metric` | `pl.Enum` | Metric name registered in the metric registry. |
| `label` | `pl.Utf8`| Human-readable display label |
| `stat` | `pl.Struct` | Typed value container (see below). |
| `context` | `pl.Struct` | Metadata describing how the metric was computed. |
| `id` | `pl.Struct` | Entity identifiers for within-subject / visit metrics. |
| `error` | `pl.Struct` | Error message if a metric failed to derive. |

The `stat` struct splits the stored result across typed channels:

- `type`: value hint such as `"float"`, `"int"`, `"bool"`, `"string"`, or `"struct"`
- `value_float`, `value_int`, `value_bool`, `value_str`, `value_struct`: mutually exclusive
  slots for the actual statistic
- `format`: optional Python format string used when rendering

# Producing ARD from an Evaluation

The evaluator returns an `EvaluationResult`, a thin Polars `DataFrame` subclass that keeps
`label`, `metric`, `estimate`, and `value` columns ready for display. 
You can always obtain its backing ARD via `to_ard()`.

```{python}
# Sample data with two estimates, grouped by treatment and gender
raw = generate_sample_data(n_subjects=4, n_visits=2, n_groups=2)
metrics = [
    MetricDefine(name="mae", label="Mean Absolute Error"),
    MetricDefine(name="rmse", label="Root Mean Squared Error"),
]

evaluator = MetricEvaluator(
    df=raw,
    metrics=metrics,
    ground_truth="actual",
    estimates=["model1", "model2"],
)

result = evaluator.evaluate()
result
```


# ARD Container

Convert the evaluation result to an ARD object:

```{python}
ard = result.to_ard()
ard
```

```{python}
ard.schema
```

Collecting produces the backward-compatible table with struct columns:

```{python}
canonical = ard.collect()
```

The typed `stat` payload is preserved even though the `value` column is purely presentational:

```{python}
canonical["stat"][0]
```

Structured columns can be unnested to inspect individual components:

```{python}
canonical.select([
    pl.col("estimate"),
    pl.col("metric"),
    pl.col("stat").struct.field("value_float").alias("value_float"),
    pl.col("stat").struct.field("format").alias("format"),
])
```

`ARD.get_stats()` offers a quick way to access raw values with optional metadata:

```{python}
ard.get_stats()
```

# Transformation

```{python}
ard.to_long()
```

```{python}
ard.unnest().head()
```

# Summaries

Use `summary()` for a quick diagnostic of the collected dataset or `describe()` for a
formatted printout:

```{python}
ard.summary()
```

```{python}
ard.describe()
```
