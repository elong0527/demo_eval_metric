---
title: "Basic Usage Example"
format: html
execute:
  echo: true
  warning: false
---

# Polars Eval Metrics - Basic Usage

This example demonstrates the basic usage of the polars-eval-metrics package for evaluating model performance.

## Setup

```{python}
import sys
sys.path.insert(0, '../src')

from polars_eval_metrics import (
    EvaluationConfig,
    MetricData,
    MetricEvaluator,
    MetricFactory,
    MetricType,
)
from data_generator import generate_sample_data
```

## 1. Generate Sample Data

First, let's generate some sample data with multiple subjects, visits, and treatment groups:

```{python}
# Generate sample data
df = generate_sample_data(n_subjects=4, n_visits=3, n_groups=2)

# Display data info
print(f"Data shape: {df.shape}")
print(f"Columns: {df.columns}")

# Show first few rows
df.head()
```

## 2. Define Metrics Programmatically

You can define metrics directly using the `MetricData` class:

```{python}
# Define metrics
metrics = [
    MetricData(
        name="mae",
        label="Mean Absolute Error",
        type=MetricType.ACROSS_SAMPLES,
    ),
    MetricData(
        name="rmse",
        label="Root Mean Squared Error",
        type=MetricType.ACROSS_SAMPLES,
    ),
    MetricData(
        name="bias",
        label="Bias",
        type=MetricType.ACROSS_SAMPLES,
        agg_expr=["pl.col('error').mean().alias('value')"],
    ),
]

print(f"Created {len(metrics)} metrics")
```

## 3. Create and Run Evaluator

Create an evaluator with the data and metrics:

```{python}
# Create evaluator
evaluator = MetricEvaluator(
    df=df,
    metrics=metrics,
    ground_truth="actual",
    estimates=["model1", "model2"],
    group_by=["treatment"],
)

# Evaluate all metrics
results = evaluator.evaluate_all()
print(f"Results shape: {results.shape}")
```

## 4. Display Results

View the results in a pivot table format:

```{python}
# Pivot results for better readability
pivot_results = results.pivot(
    values="value",
    index=["treatment", "metric"],
    on="estimate",
)

pivot_results
```

## 5. Using YAML Configuration

You can also define the entire configuration in a dictionary (or YAML file):

```{python}
# Define configuration
config_dict = {
    "ground_truth": "actual",
    "estimates": ["model1", "model2"],
    "group_by": ["treatment", "gender"],
    "metrics": [
        {"name": "mae", "label": "MAE", "type": "across_samples"},
        {"name": "rmse", "label": "RMSE", "type": "across_samples"},
        {
            "name": "mae_per_subject",
            "label": "MAE per Subject",
            "type": "across_subject",
            "agg": {"expr": "pl.col('absolute_error').mean().alias('value')"},
            "select": {"expr": "pl.col('value').mean()"},
        },
    ],
}

# Load configuration
config = EvaluationConfig.from_yaml(config_dict)
print(f"Loaded {len(config.metrics)} metrics from config")
```

## 6. Evaluate with Configuration

Create an evaluator from the configuration:

```{python}
# Create evaluator from config
evaluator_from_config = MetricEvaluator(
    df=df,
    **config.to_evaluator_kwargs()
)

# Evaluate
config_results = evaluator_from_config.evaluate_all()
print(f"Results shape: {config_results.shape}")
```

## 7. Results by Treatment and Gender

Display results grouped by treatment and gender:

```{python}
# Show first 10 rows of results
config_results.select(["treatment", "gender", "metric", "estimate", "value"]).head(10)
```

## Summary

This example demonstrated:
- Generating sample data for testing
- Defining metrics programmatically
- Creating an evaluator and running evaluations
- Using YAML-based configuration
- Viewing results in different formats

The package supports various metric types, custom expressions, and flexible grouping options for comprehensive model evaluation.